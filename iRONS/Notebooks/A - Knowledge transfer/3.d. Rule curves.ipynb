{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimisation of the reservoir rule curve\n",
    "In this Notebook we will see why and how to optimise the rule curve of a water reservoir system. A rule curve is the representation of the different reservoir operating policies across the year. The year is divided into discrete intra-year time periods, each period having a specified reservoir operating policy, i.e a function that returns the release volume based on the conditions of the reservoir system (for instance, the reservoir storage, the demand forecast, the time of year, etc.) at that time. Equally as the operating policy, the rule curve is optimised once and then applied forever after (or at least, until a revision of the policy is needed) and the optimisation of the rule curve maximises the long-term benefits. These can be estimated by using (sufficiently long) historical time series or model projections.\n",
    "\n",
    "<left><img src=\"../../util/images/Dam4.gif\" width = \"500px\"><left>\n",
    "    \n",
    "Once again we consider a simple illustrative system where a reservoir is operated to supply water to a domestic consumption node, while ensuring a minimum environmental flow in the downstream river (also called “environmental compensation flow”) and maintaining the water level in the reservoir within prescribed limits. We use a mathematical model to link all the key variables that represent the reservoir dynamics (inflow, storage and outflows) and use model simulation/optimisation to determine the reservoir rulve curve that optimizes the **long-term** (several years) system performance. We use the historical time series of inflows and water demand to estimate such long-term performance. The underpinning assumption here is that the system forcings observed over the past years are representative of the forcings that will drive the system in the future (if this assumption is not sensible, for instance because of ongoing changes that will likely impact the hydrological regime or demand pattern, then one may use model projections of inflow and demand in place of historical observations) \n",
    "<left> <img src=\"../../util/images/system_representation_IO1.png\" width = \"600px\"><left>\n",
    "\n",
    "We will use a simple form of rule curve where the transition between periods is step-wise and where the reservoir release is a only determined by the storage value, as in the Figure below. Higher storage values are associated to higher releases, which is useful for flood control purposes, whereas at low storage values less water is released to reduce the risk of future water shortages [(Loucks et al., 1981)](https://link.springer.com/book/10.1007/978-3-319-44234-1).\n",
    "<left><img src=\"../../util/images/Rule_curve.png\" width = \"800px\"><left>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries\n",
    "To run this notebook we need to import some libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bqplot import pyplot as plt\n",
    "from bqplot import *\n",
    "from bqplot.traits import *\n",
    "import numpy as np\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "from platypus import NSGAII, Problem, Real, Integer # Import the optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the shape of the operating policy\n",
    "As anticipated, in this notebook we will use an operating policy that determines the reservoir release based on the storage value only. In particular, we will use a piece-wise linear function of the storage. The function is implemented in a series of submodules, that we can import with the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Modules.Interactive_rule_curves import Interactive_rule_curve_manual, Interactive_rule_curve_auto\n",
    "from irons.Functions.Data_management.Read_data import read_csv_data\n",
    "from irons.Functions.Data_management.day2week2month import day2week\n",
    "from irons.Functions.Reservoir_operating_policy.Operating_policy import policy_function\n",
    "from irons.Functions.Reservoir_operating_policy.Operating_rule_curves import rule_curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our code, the operating policy uses rescaled release and storage values. Specifically, the storage is scaled by the reservoir active capacity, so that in the operating policy function it varies between 0 (dead storage) and 1 (full storage). The release is scaled with respect to its mean, so that the operating policy returns the fraction (or multiple) of the mean release. \n",
    "The piece-wise linear function representing the operating policy is delineated by 4 points (x0, x1, x2 and x3): the minimimum and maximum (storage,release) points (x0 and x3 respectively), and two inflection points (x1, x2) where the slope of the function changes. The function returns a constant release (for instance, the target demand) when the storages stay between points x1 and x2; the release is reduced if the storage is below x1, or increased if it goes above x2. Let's now attribute a (tentative) value to the coordinates of these points, so that we can visualise the operating policy. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# System characteristics\n",
    "d             = 15 # ML/week - water demand (we assume as constant) \n",
    "env_min       = 4 # ML/week - environmental compensation flow\n",
    "Qreg_rel_mean = d # ML/week - the (long-term) mean release = demand  \n",
    "\n",
    "# System constraints\n",
    "Qreg_rel_min = env_min # ML/week - the release at minimum storage \n",
    "Qreg_rel_max = 40 # ML/week - the maximum release capacity \n",
    "\n",
    "s_min   = 0 # ML - minimum storage (set to zero for now)\n",
    "s_max   = 150 #  ML - maximum storage (=reservoir capacity) \n",
    "\n",
    "### Rule curve parameters ###\n",
    "# Points defining the curves \n",
    "# (linear interpolation will be applied to define the curves between the points)\n",
    "curve_dates = ['1 Apr',    '1 Aug',    '1 Dec',    '31 Mar']\n",
    "s_0         = [s_min/s_max,s_min/s_max,s_min/s_max,s_min/s_max]\n",
    "s_1         = [0.6,        0.3,        0.2,        0.6] \n",
    "s_2         = [0.9,        0.6,        0.5,        0.9] \n",
    "s_3         = [s_max/s_max,s_max/s_max,s_max/s_max,s_max/s_max]\n",
    "# Points defining the operating policy across the year \n",
    "# (linear interpolation will be applied to define the curves between the points)\n",
    "rule_dates  = ['21 Mar',     '21 Jun',     '21 Sep',     '21 Dec']\n",
    "u_0         = [Qreg_rel_min, Qreg_rel_min, Qreg_rel_min, Qreg_rel_min]\n",
    "u_1         = [Qreg_rel_mean,Qreg_rel_mean,Qreg_rel_mean,Qreg_rel_mean]\n",
    "u_2         = [Qreg_rel_mean,Qreg_rel_mean,Qreg_rel_mean,Qreg_rel_mean]\n",
    "u_3         = [Qreg_rel_max, Qreg_rel_max, Qreg_rel_max, Qreg_rel_max]\n",
    "\n",
    "param = {'curves':{'year_date'    : curve_dates,\n",
    "                   'storage_frac' : [s_0,s_1,s_2,s_3]},\n",
    "\n",
    "         'rules' :{'year_date' : rule_dates,\n",
    "                   'release'   : [u_0,u_1,u_2,u_3]}} # max release\n",
    "s_yday,r_yday = rule_curve(param)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now plot the rule curve with the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d65c053e9a484b05b34ddbeb2e28eb7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Figure(axes=[Axis(grid_lines='none', label='day of the year', scale=LinearScale()), Axis(grid_l…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### Axis characteristics ###\n",
    "x_sc_0 = LinearScale(); y_sc_0 = LinearScale(min=0,max=1)\n",
    "x_ax_0 = Axis(label='day of the year', scale=x_sc_0, grid_lines = 'none')\n",
    "y_ax_0 = Axis(label='storage fraction', scale=y_sc_0, orientation='vertical', grid_lines = 'none')\n",
    "### Plot ###\n",
    "rule_curve_1 = Lines(x = np.arange(1,367), y = s_yday[1],\n",
    "                     colors=['blue'], stroke = 'lightgray',\n",
    "                     scales={'x': x_sc_0, 'y': y_sc_0},\n",
    "                     fill   = 'top',fill_opacities = [1],fill_colors = ['blue'])\n",
    "rule_curve_2 = Lines(x = np.arange(1,367), y = s_yday[2],\n",
    "                     colors=['blue'], stroke = 'lightgray',\n",
    "                     scales={'x': x_sc_0, 'y': y_sc_0},\n",
    "                     fill   = 'top',fill_opacities = [1],fill_colors = ['lightblue'])\n",
    "### Figure characteristics ###\n",
    "fig_0 = plt.Figure(marks = [rule_curve_1,rule_curve_2],title = 'Rule curves', axes=[x_ax_0, y_ax_0],\n",
    "                   layout={'width': '800px', 'height': '450px'}, legend_style = {'fill': 'white', 'opacity': 0.5},\n",
    "                   background_style = {'fill': 'darkblue'})\n",
    "widgets.VBox([fig_0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimising the operating policy by trial and error (manual optimisation)\n",
    "In this section we will refine the parameters of the operating policy (that is, in our example, the coordinates of points x0,x1,x2,x3) by trying to increase the system performance when simulated against the historical inflows, evaporation and water demand data. \n",
    "### Loading historical inflows, evaporation and water demand data\n",
    "Let's assume we want to look at 100 weeks from 2014 to 2015, and load the evaporation, inflow and demand observations for this period from a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 100 # weeks\n",
    "### Load evaporation data ###\n",
    "inputs_folder_path = 'Inputs/'\n",
    "clim_data_file = 'clim_data_2014_15.csv'\n",
    "date_day, clim_data_day  = read_csv_data(inputs_folder_path, clim_data_file)\n",
    "date, e_data_week, e_data_week_cum = day2week(N,date_day,clim_data_day[:,0])\n",
    "### Load inflow data ###\n",
    "I_data_file = 'inflow_data_2014_15.csv'\n",
    "date_day, I_data_day = read_csv_data(inputs_folder_path, I_data_file)\n",
    "date, I_data_week, I_data_week_cum = day2week(N,date_day,I_data_day[:,0])\n",
    "### Load demand data ###\n",
    "d_data_file = 'demand_data_2014_15.csv'\n",
    "date_day, d_data_day = read_csv_data(inputs_folder_path, d_data_file)\n",
    "date, d_data_week, d_data_week_cum = day2week(N,date_day,d_data_day)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the inflow time series:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a2ea24b1fdd4bd8ae884e06bef6d822",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Figure(axes=[Axis(label='week', scale=DateScale()), Axis(label='ML/week', orientation='vertical…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Axis characteristics\n",
    "x_sc_1 = DateScale();y_sc_1 = LinearScale(min=0,max=40)\n",
    "x_ax_1 = Axis(label='week', scale=x_sc_1);y_ax_1 = Axis(label='ML/week', scale=y_sc_1, orientation='vertical')\n",
    "# Bar plot\n",
    "evap_plot = plt.bar(date,e_data_week[:,0],colors=['green'],stroke = 'lightgray',scales={'x': x_sc_1, 'y': y_sc_1},\n",
    "                      labels = ['evaporation'], display_legend = True)\n",
    "inflow_plot = plt.bar(date,I_data_week[:,0],colors=['blue'],stroke = 'lightgray',scales={'x': x_sc_1, 'y': y_sc_1},\n",
    "                      labels = ['inflow'], display_legend = True)\n",
    "# Figure characteristics\n",
    "fig_1 = plt.Figure(marks = [inflow_plot,evap_plot],title = 'Inflow', axes=[x_ax_1, y_ax_1],\n",
    "                    layout={'min_width': '900px', 'max_height': '300px'}, legend_style = {'fill': 'white', 'opacity': 0.5})\n",
    "widgets.VBox([fig_1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the demand time series:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7db97f959733432b98dafc24a4cf3869",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Figure(axes=[Axis(label='week', scale=DateScale()), Axis(label='ML/week', orientation='vertical…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Bar plot (we use the same axis as the weekly inflows figure)\n",
    "demand_plot   = plt.bar(date,d_data_week[:,0],colors=['gray'],stroke = 'lightgray',opacities = [0.7]*(N+1), \n",
    "                        labels = ['demand'], display_legend = True, \n",
    "                    stroke_width = 1,scales={'x': x_sc_1, 'y': y_sc_1})\n",
    "#Figure characteristics\n",
    "fig_1b = plt.Figure(marks = [demand_plot],title = 'Demand', axes=[x_ax_1, y_ax_1],\n",
    "                    layout={'min_width': '900px', 'max_height': '300px'}, legend_style = {'fill': 'white', 'opacity': 0.5})\n",
    "widgets.VBox([fig_1b])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definition of reservoir simulation function and initial storage\n",
    "Next, we need to import the iRONs function that implements the reservoir simulation (this function iteratively applies the mass balance equation so to reconstruct the temporal evolution of the reservoir variables over the simulation period). We also need to specify the initial storage volume to start the simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Import the reservoir simulation function ###\n",
    "from irons.Functions.Reservoir_system_simulation.Res_sys_sim import Res_sys_sim\n",
    "### Define initial storage for simulation ###\n",
    "s_ini = 100 # ML - initial storage volume"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definition of the system objectives\n",
    "Last, we need to define the objectives that we want to be maximised by the operating policy. As anticipated in the Introduction of this Notebook, our illustrative reservoir is operated to support domestic supply while maintaining the reservoir level above a prescribed target (this could be, for example, because the quality of the water deteriorates when levels are low, requiring more costly treatment).\n",
    "We will pursue the first objective by minimising the following Total Squared Deficit (TDC) with respect to the historical water demand:\n",
    "\n",
    "$$TSD = \\sum_{t=1}^{N} [ \\ max( \\ 0, \\ d(t)-u(t) \\ ) \\ ]^2 $$\n",
    "\n",
    "where N is the length of the simulation period and d(t) is the water demand for each time-interval in that period. Notice that the function $max(0,...)$ enables us to only count the difference between demand d and release u when this is positive, that is, when the release u is smaller than the demand d, and a water shortage is indeed produced. Also, the squaring is a 'mathematical trick' to make sure that larger deficit amounts are given more weight than smaller ones. This translates the fact that small deficit amounts are easier to mitigate and hence more acceptable, while larger ones can cause disproportionately severe impacts and should be avoided as much as possible.\n",
    "\n",
    "We are also interested in minimising the chances that the reservoir level goes below a minimum threshold. We measure how well this criterion is satisfied by the following Critical Storage Violation (CSV) function:\n",
    "\n",
    "$$CSV = \\sum_{t=1}^{N} [ \\ max ( \\ cs - s(t) , \\ 0) \\ ] $$\n",
    "\n",
    "where, again, N is the length of the simulation period, s is the reservoir storage, and cs is the minimum reservoir storage threshold that should preferably not be transpassed. For our case, let's set this threshold to 40 ML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cs = np.array([40]*(N+1)) # (ML)  minimum reservoir storage threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determining the optimal operating policy via interactive visualisation\n",
    "\n",
    "Now use the sliders to modify the parameters of the operating policy in a way that minimises the Total Squared Deficit (TSD) and the Minimum Storage Violation (MSV)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e482f6a3beff46ae8591d51a609a410f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(HBox(children=(FloatSlider(value=0.6, continuous_update=False, description='s1 a…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "d = d + np.zeros(N)\n",
    "fig_1a,fig_1b,fig_1c,s_1_0_slider,s_1_1_slider,s_1_2_slider = Interactive_rule_curve_manual(\n",
    "                                                                           Res_sys_sim,policy_function,rule_curve,\n",
    "                                                                           date,\n",
    "                                                                           I_data_week[1:,0],e_data_week[1:,0], \n",
    "                                                                           s_ini, s_min, s_max, \n",
    "                                                                           Qreg_rel_mean, Qreg_rel_min, Qreg_rel_max,\n",
    "                                                                           cs, d)\n",
    "\n",
    "Box_layout = widgets.Layout(justify_content='center')\n",
    "widgets.VBox([widgets.HBox(\n",
    "    [widgets.HBox([s_1_0_slider,s_1_1_slider,s_1_2_slider],\n",
    "                  layout=Box_layout), fig_1a],layout=Box_layout),fig_1b,fig_1c],layout=Box_layout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From manual to automatic optimization \n",
    "As we have seen, when we deal with two conflicting objectives, we cannot find a solution that optimise both simoultaneously. If we prioritize one objective, the other one is deteriorated: there is a trade-off between the two. It would then be interesting to explore this tradeoff, and find a set of operating policies that produce different optimal combinations of the two objectives. However, this is too cumbersome to do manually, so we can use a multi-objective optimisation algorithm to do that for us. The algorithm will automatically test a very large number of combinations of the policy parameters u_ref, s_ref_1, s_ref_2, until it finds a set of combinations that realise (approximately) optimal tradeoffs.\n",
    "\n",
    "Here we use a multi-objective optimisation algorithm called NSGAII, which is implemented in the Python Platypus package. For more information about these methods and tools, see [Deb et al, 2002](https://ieeexplore.ieee.org/document/996017) and the [Platypus webpage](https://platypus.readthedocs.io). The code to run the optimisation is the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_2_inc = 0.3\n",
    "def auto_optim(vars):\n",
    "    \n",
    "    s_1 = [vars[0],vars[1],vars[2],vars[0]]\n",
    "    s_2 = [s_1[0]+s_2_inc,s_1[1]+s_2_inc,s_1[2]+s_2_inc,s_1[3]+s_2_inc]\n",
    "    \n",
    "    param = {'curves':{'year_date'    : curve_dates,\n",
    "                       'storage_frac' : [s_0,s_1,s_2,s_3]},\n",
    "\n",
    "             'rules' :{'year_date' : rule_dates,\n",
    "                       'release'   : [u_0,u_1,u_2,u_3]}} # max release\n",
    "\n",
    "    Qreg = {'releases' : {'type'  : 'rule curve',\n",
    "                          'input' : [policy_function, rule_curve],\n",
    "                          'param' : param},\n",
    "            'inflows' : [],\n",
    "            'rel_inf' : []}\n",
    "    \n",
    "    Qenv, Qspill, u, I_reg, s, E = Res_sys_sim(date[1:],\n",
    "                                               I_data_week[1:,0], e_data_week[1:,0], \n",
    "                                               s_ini, s_min, s_max, \n",
    "                                               env_min, d, \n",
    "                                               Qreg)\n",
    "    \n",
    "    TSD = (np.sum((np.maximum(d_data_week-u,[0]*N))**2)).astype('int')\n",
    "    CSV = (np.sum((np.maximum(cs-s,[0]*(N+1))))).astype('int')\n",
    "    \n",
    "    return [TSD, CSV]\n",
    "\n",
    "problem = Problem(3,2)\n",
    "Real0   = Real(0, 1-s_2_inc); Real1 = Real(0, 1-s_2_inc); Real2 = Real(0, 1-s_2_inc)\n",
    "\n",
    "problem.types[:]       = [Real0] + [Real1] + [Real2]\n",
    "problem.function       = auto_optim\n",
    "\n",
    "population_size = 50\n",
    "algorithm       = NSGAII(problem,population_size)\n",
    "algorithm.run(500) # Number of iterations\n",
    "\n",
    "results1_optim = np.array([algorithm.result[i].objectives[0] for i in range(population_size)])\n",
    "results2_optim = np.array([algorithm.result[i].objectives[1] for i in range(population_size)])\n",
    "\n",
    "sol_optim = [algorithm.result[i].variables for i in range(population_size)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot the optimisation results\n",
    "We can visualise the tradeoffs between the two objectives in one plot, called Pareto front, which displays the combination of the two objective values in correspondence to a set of optimised solutions. Click on one point in the Pareto front to visualise the operating policy that generates that performance, and associated storage time series.  What do you think would be a balanced solution?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3adc83f2fd0b4767a58bd2cd7ab65034",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(Figure(animation_duration=1000, axes=[Axis(label='Total squared deficit [ML^2]',…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig_pf, fig_2a,fig_2b,fig_2c = Interactive_rule_curve_auto(Res_sys_sim,policy_function,rule_curve,\n",
    "                                                           date,\n",
    "                                                           I_data_week[1:,0],e_data_week[1:,0], \n",
    "                                                           s_ini, s_min, s_max, \n",
    "                                                           Qreg_rel_mean, Qreg_rel_min, Qreg_rel_max,\n",
    "                                                           cs, d,\n",
    "                                                           results1_optim,results2_optim,sol_optim)\n",
    "\n",
    "Box_layout = widgets.Layout(justify_content='center')\n",
    "widgets.VBox([widgets.HBox(\n",
    "    [fig_pf, fig_2a],layout=Box_layout),fig_2b,fig_2c],layout=Box_layout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References \n",
    "\n",
    "Deb K. et al (2002) A fast and elitist multiobjective genetic algorithm: NSGA-II, IEEE Transactions on Evolutionary Computation, 6(2), 182-197, doi:10.1109/4235.996017.\n",
    "\n",
    "Dobson B. et al (2019) An argument-driven classification and comparison of reservoir operation optimization methods, Advances in Water Resources, 128, 74-86.\n",
    "\n",
    "Loucks D. P. et al (1981) Water resource systems planning and analysis, Prentice-Hall."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Questionnaire: Section 2 of 2\n",
    "Now that you are done with the Notebooks, could you please answer the questions of Section 2 of 2 of the questionnaire (click on Next button that you will find at the end of Section 1 of 2 of the questionnaire)?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
